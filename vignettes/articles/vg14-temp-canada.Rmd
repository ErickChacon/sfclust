---
title: "Standardized daily temperature at Canadian stations (1960–1994)"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Standardized daily temperature at Canadian stations (1960–1994)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.align = "center", eval = TRUE)
knitr::opts_chunk$set(fig.height = 6.5, fig.width = 7.5, dpi = 90, out.width = '100%')
knitr::opts_chunk$set(comment = "#>")
```

In this vignette, we use `sfclust` to cluster Canadian stations with similar standardized
daily temperatures averaged over the period 1960–1994.

## Load packages and data

```{r, warning = FALSE, message = FALSE}
library(sfclust)
library(stars)
library(ggplot2)
library(dplyr)
library(rnaturalearth)
```

We will use the polygonal shapes of the Canada states using the `rnaturalearth` package,
and the common dataset `CanadianWeather` for functional data analysis from the `fda`
package. Notice that this is a list containing the daily data per stations, and other
information such as the location of the stations (`coordinates`).

```{r}
canada <- ne_states("Canada")
data(CanadianWeather, package = "fda")
names(CanadianWeather)
```

## Prepare data

### Create stars data

Firt, we will convert the stations to the class `sf`, and visualize their locations.

```{r}
stations <- as.data.frame(CanadianWeather$coordinates) |>
  mutate(longitud = - `W.longitude`) |>
  rename(latitud = "N.latitude") |>
  select(longitud, latitud) |>
  st_as_sf(coords = c("longitud", "latitud"), crs = st_crs(4326))

ggplot() +
  geom_sf(data = canada) +
  geom_sf(data = stations, size = 2) +
  theme_bw()
```

We standardised the temperature per station to be able to clusterize based on the
functional form rather than the absolute value. Then, we convert the data to a stars
object with dimensions `geometry` and `time` as expected by `sfclust`.

```{r}
time <- seq(as.Date("1977-01-01"), as.Date("1977-12-31"), by = "1 day")
canweather <- st_as_stars(
    temp = t(CanadianWeather$dailyAv[, , 1]),
    ztemp = t(scale(CanadianWeather$dailyAv[, , 1])),
    dimensions = st_dimensions(geometry = st_geometry(stations), time = time, point = TRUE)
)
canweather
```

```{r, echo = FALSE}
saveRDS(canweather, file = here::here(file.path("tools", "data", "canweather.rds")))
```

### Exploratory analysis

Let's visualize the temperature averaged per month. We can observe that higher values of
standardised temperature have been observed in the north stations on July; while the lowest values
have been observed on January around some stations located around the south areas closed
to the center.

```{r, fig.height = 6}
monthdata <- aggregate(canweather, by = "month", FUN = mean)
ggplot() +
  geom_sf(data = canada) +
  geom_stars(aes(fill = ztemp), monthdata, shape = 21, size = 1.3) +
  facet_wrap(~ time) +
  scale_fill_distiller(palette = "RdBu") +
  theme_bw() +
  theme(legend.position = "bottom")
```

Additionally we can observe the temperal trends per regions. Although the functional
shapes seem similar, it is important to notice that the temperature increases and decays at different
times of the year per station.

```{r, fig.height = 5.5}
canweather |>
  st_set_dimensions("geometry", values = 1:nrow(canweather)) |>
  as_tibble() |>
    ggplot() +
    geom_line(aes(time, ztemp, group = geometry, color = factor(geometry)), linewidth = 0.3) +
    theme_bw() +
    theme(legend.position = "none")
```

## Spatial clustering

### Create graph

Our function `sfclust()` assumes that the `geometry` type of the provided `stars` object if `POLYGON`,
from which it creates an `igraph` object using the adjacency matrix. However, the processed
`canweather` has `POINT` as a geometry representing the locations of stations. In this
case, it is necessary to define a graph that captures the connectiviy to consider between
stations. For this, we will create a Voronoi tessellation with will create a polygon for
each station.

```{r, fig.height = 5}
stations2 <- st_transform(stations, st_crs(3857))

# create boundary
boundary <- st_convex_hull(st_union(stations2)) |>
  st_buffer(units::set_units(1000, "km"))

# create polygons with voronoi
domain <- st_cast(st_voronoi(st_union(stations2), boundary)) |>
  st_intersection(boundary)

# reorganize the polygons to match stations
domain <- domain[as.numeric(st_within(stations2, domain))] |>
  st_transform(st_crs(stations))

ggplot() +
  geom_sf(data = domain, color = 2, fill = NA) +
  geom_sf(data = stations)
```

### Model fitting

In this application, because of the trends observed before, we will assume that the mean
standardised temperature can be explained by (i) a polynomial trend with respect to
`time`, (ii) and an autoregressive process over times units `idt` to capture correlated
temporal variation. Additionally, we will start assuming that each state is a cluster,
such us we will start with 35 clusters using the created polygonal data  (`domain`).
Because of the presence of strong noise on the curves, we will use a strong penalty (-300)
for the increase of parameters, it means that to have high chances of accepting the
creation a new cluster, then the log marginal likelihood has be to improve by a value of
300, if the proposal has an improvement on the marginal likelihood smaller than 300, it
will probably be rejected.

```{r, eval = FALSE, echo = FALSE}
# formula <- ztemp ~ poly(time, 2) + f(idt, model = "rw1")
# geodata <- genclust(domain, nclust = 35)
#
# set.seed(123)
# result <- sfclust(canweather, graphdata = geodata, formula = formula,
#     logpen = -300, niter = 4000, burnin = 0, thin = 10, nmessage = 10, nsave = 100,
#     control.inla = list(control.vb = list(enable = FALSE)),
#     path_save = file.path("tools", "data", "canweather-mcmc-test.rds"))
```

```{r, eval = FALSE}
formula <- ztemp ~ poly(time, 2) + f(idt, model = "rw1")
geodata <- genclust(domain, nclust = 35)

set.seed(123)
result <- sfclust(canweather, graphdata = geodata, formula = formula,
    logpen = -300, niter = 4000, burnin = 0, thin = 10, nmessage = 10, nsave = 100,
    control.inla = list(control.vb = list(enable = FALSE)),
    path_save = file.path("canweather-mcmc.rds"))
result
```

```{r, echo = FALSE}
formula <- ztemp ~ poly(time, 2) + f(idt, model = "rw1")
geodata <- genclust(domain, nclust = 35)
result <- readRDS(here::here(file.path("tools", "data", "canweather-mcmc.rds")))
result
```

Notice that 8 times there was an increase of number of clusters, 29 times clusters were
merged, 19 times the composition were modified, and 171 times the minimum spanning tree has
been modified. Also after thinning we keep 400 samples from a total of 4000, and the
marginal likelihood achieve a value of 17541.96.

### Results

```{r}
summary(result, sort = TRUE)
```

The `summary` indicates that 8 clusters from 14 have more than one member, where the two
biggest cluster includes 7 stations, the third 4, and so on. In order to verify the
adequacy of this clustering, we check the convergence using the `plot()` function with
option `which = 3`.


```{r, fig.height = 4.5}
plot(result, which = 3)
```

The figure indicates that the marginal likelihood improves significantly within the first
100 iterations (after thinning) and seems afterward. Now, we can visualize the spatial
cluster assignment and the predicted mean for each cluster:

```{r, fig.height = 3.2}
gg1 <- plot_clusters_map(result, sort = TRUE, legend = TRUE, clusters = 1:8,
    geom_before = geom_sf(data = canada), size = 3, alpha = 0.8)
gg2 <- plot_clusters_fitted(result, sort = TRUE, clusters = 1:8, linewidth = 0.4)
gg1 + gg2
```

```{r, echo = FALSE}
path_figures <- here::here(file.path("tools", "figures"))
ggsave(file.path(path_figures, "canweather-plot.pdf"), width = 10, height = 4.1, device = cairo_pdf)
```

We can observe that the biggest cluster is located on the southeast of Canada with the total
of 7 close stations, while the second biggest is located on the south conformed by 7
stations that are more further apart. The third one, with 4 stations, is located on the
southwest also more further apart.

### Empirical standardised temperature per cluster

We can use that information of the cluster assignment to visualize the empirical data per
cluster using the function `plot_cluster_series()`. In this case we show that we can
easily customize our resulting graph using `ggplot2` functions.

```{r, fig.height = 4.5}
plot_clusters_series(result, ztemp) +
  geom_hline(yintercept = 0, linetype = 2) +
  facet_wrap(~ cluster, ncol = 5) +
  scale_x_date(date_breaks = "2 months", date_labels =  "%b") +
  labs(y = "Standardized temperature")
```

```{r, echo = FALSE}
ggsave(file.path(path_figures, "canweather-risk-per-cluster.pdf"), width = 10, height = 5.5, device = cairo_pdf)
```

Panels 1-8, shows the empirical risk per cluster for those who have more than 1 stations.
The main differences between these clusters if the initial shape, the speed of increase,
the peak shape and time, the decay behaviour. For example clusters 1 and 2 has a bell
shape, while others like 3-6 have almost a linear increase until reaching a maximum level.
Panels 9-14 shows the empirical risk per cluster for those who have only 1 station. Notice
that these clusters have more particular shapes that differentiate from the previous
clusters, and between them.

```{r, echo = FALSE}
# pred <- fitted(result, sort = TRUE, aggregate = TRUE)
# ggplot() +
#     geom_stars(aes(fill = mean_cluster), data = pred) +
#     facet_wrap(~ time) +
#     scale_fill_distiller(palette = "RdBu") +
#     labs(title = "Daily risk", fill = "Risk") +
#     theme_bw(base_size = 7) +
#     theme(legend.position = "bottom")
```

```{r, echo = FALSE}
# # saveRDS(list(x = 1, b = 2), "find-problem.rds")
#
#  # -19550.807576937
# plot(result)
#  # -19449.8533593845
#  # -19306.77
#  # -19284.15
#
# # result2 =  update(result, niter = 2000, path_save = file.path("inst", "vigdata", "usacovid2.rds"))
#
# result = readRDS("clust_usacovid1.rds")
# # result2 =  readRDS(file.path("inst", "vigdata", "usacovid2.rds"))
# summary(result)
# # summary(result2)
# # library(sfclust)
# plot(result, which = 1)
```

